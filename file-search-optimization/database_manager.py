"""
Database Manager with Cosine Similarity Deduplication
Generated by Archie - Autonomous Agent

Purpose: Finds and removes duplicate embeddings to save storage
Impact: Faster searches, lower costs
Risk Level: LOW
"""

import numpy as np
from typing import List, Tuple, Set
from sklearn.metrics.pairwise import cosine_similarity


class VectorDatabaseManager:
    """Manages vector embeddings with deduplication"""

    def __init__(self, similarity_threshold: float = 0.98):
        """
        Initialize database manager.

        Args:
            similarity_threshold: Cosine similarity threshold for duplicates (default 0.98)
        """
        self.similarity_threshold = similarity_threshold
        self.embeddings: List[np.ndarray] = []
        self.metadata: List[dict] = []
        self.embedding_ids: List[str] = []

    def add_embedding(self, embedding: np.ndarray, metadata: dict, embedding_id: str) -> bool:
        """
        Add embedding to database, checking for duplicates.

        Args:
            embedding: Embedding vector
            metadata: Associated metadata
            embedding_id: Unique ID

        Returns:
            True if added, False if duplicate
        """
        # Check if already exists by ID
        if embedding_id in self.embedding_ids:
            return False

        # Check for near-duplicates
        if len(self.embeddings) > 0:
            if self._is_duplicate(embedding):
                return False

        # Add to database
        self.embeddings.append(embedding)
        self.metadata.append(metadata)
        self.embedding_ids.append(embedding_id)

        return True

    def _is_duplicate(self, new_embedding: np.ndarray) -> bool:
        """
        Check if embedding is duplicate of existing ones.

        Args:
            new_embedding: Embedding to check

        Returns:
            True if duplicate found
        """
        if len(self.embeddings) == 0:
            return False

        # Calculate cosine similarity with all existing embeddings
        existing = np.array(self.embeddings)
        new = new_embedding.reshape(1, -1)

        similarities = cosine_similarity(new, existing)[0]

        # Check if any similarity exceeds threshold
        return np.any(similarities >= self.similarity_threshold)

    def find_duplicates(self) -> List[Tuple[int, int, float]]:
        """
        Find all duplicate pairs in database.

        Returns:
            List of (index1, index2, similarity) tuples
        """
        if len(self.embeddings) < 2:
            return []

        duplicates = []
        embeddings_array = np.array(self.embeddings)

        # Calculate pairwise similarities
        similarities = cosine_similarity(embeddings_array)

        # Find pairs above threshold (excluding diagonal)
        for i in range(len(similarities)):
            for j in range(i + 1, len(similarities)):
                if similarities[i, j] >= self.similarity_threshold:
                    duplicates.append((i, j, similarities[i, j]))

        return duplicates

    def remove_duplicates(self) -> int:
        """
        Remove duplicate embeddings from database.

        Returns:
            Number of duplicates removed
        """
        duplicates = self.find_duplicates()

        if not duplicates:
            return 0

        # Get indices to remove (keep first occurrence)
        indices_to_remove: Set[int] = set()
        for i, j, sim in duplicates:
            indices_to_remove.add(j)  # Remove second occurrence

        # Remove in reverse order to maintain indices
        for idx in sorted(indices_to_remove, reverse=True):
            del self.embeddings[idx]
            del self.metadata[idx]
            del self.embedding_ids[idx]

        return len(indices_to_remove)

    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[str, dict, float]]:
        """
        Search for similar embeddings.

        Args:
            query_embedding: Query vector
            top_k: Number of results to return

        Returns:
            List of (id, metadata, similarity) tuples
        """
        if len(self.embeddings) == 0:
            return []

        # Calculate similarities
        embeddings_array = np.array(self.embeddings)
        query = query_embedding.reshape(1, -1)
        similarities = cosine_similarity(query, embeddings_array)[0]

        # Get top-k indices
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        # Return results
        results = [
            (self.embedding_ids[i], self.metadata[i], similarities[i])
            for i in top_indices
        ]

        return results

    def get_stats(self) -> dict:
        """Get database statistics"""
        return {
            'total_embeddings': len(self.embeddings),
            'total_ids': len(self.embedding_ids),
            'similarity_threshold': self.similarity_threshold,
            'embedding_dimensions': self.embeddings[0].shape[0] if self.embeddings else 0
        }


if __name__ == '__main__':
    # Test
    print("Testing vector database manager with deduplication...")

    db = VectorDatabaseManager(similarity_threshold=0.98)

    # Generate test embeddings
    np.random.seed(42)

    print("\nAdding 10 embeddings (including duplicates)...")
    embeddings_to_add = []
    for i in range(10):
        if i in [5, 7]:  # Make these duplicates of earlier ones
            emb = embeddings_to_add[i-3].copy()  # Duplicate
        else:
            emb = np.random.randn(300)
        embeddings_to_add.append(emb)

        added = db.add_embedding(emb, {'text': f'Document {i}'}, f'doc_{i}')
        status = "Added" if added else "Skipped (duplicate)"
        print(f"  doc_{i}: {status}")

    stats = db.get_stats()
    print(f"\nDatabase stats:")
    for key, value in stats.items():
        print(f"  {key}: {value}")

    # Find and remove duplicates
    print(f"\nFinding duplicates...")
    duplicates = db.find_duplicates()
    print(f"  Found {len(duplicates)} duplicate pairs")

    # Search test
    print(f"\nTesting search...")
    query = np.random.randn(300)
    results = db.search(query, top_k=3)
    print(f"  Top 3 results:")
    for id, meta, sim in results:
        print(f"    {id}: {meta['text']} (similarity: {sim:.4f})")
